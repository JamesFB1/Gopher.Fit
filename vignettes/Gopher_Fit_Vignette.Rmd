---
title: "Gopher.Fit Vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Gopher_Fit_Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo = FALSE, message=FALSE}
library(Gopher.Fit)
library(tidyverse)
library(patchwork)
load("~/Gopher.Fit/data/tortoise.rda")
```


# Gopher.Fit
Gopher.Fit is an R package that can be used to fit the Poisson Generalized Linear Mixed Effects Model. It implements bootstrapping to compute standard errors and confidence intervals, as well as hypothesis testing.

## Introduction to the Poisson GLMM

Like any model within the framework of GLMM, the Poisson GLMM satisfies the following 3 components:
1. The distribution of the response variable belongs to the exponential family. 
2. The mean is an invertible function of the linear predictor. This function is called the log link function. 
3. The linear predictor is a linear combination of fixed effects and random effects. 

The Poisson distribution arises naturally in many ways:
* For small success probabilities and large number of trials, Poisson is a good approximation for Binomial. 
* It arises in the case when the probability of an occurrence of an event in a given time interval is proportional to the length of the interval and independent of the occurrence of other events. In this case, the number of events in any unspecified interval is Poisson distributed. 
* Finally, it may arise naturally when the time between events is independent and exponentially distributed, and we count the number of events in a given period of time. 

The Poisson Model is defined such that

$$
\left. Y_i  \right\vert \mu_i \sim \text{Poisson}(\mu_i), \text{ } i = 1, ..., n; j = 1, ..., m
$$
where $\mu_i$ is the rate parameter (the rate by which we observe the count), and $Y_i$ can take on discrete values. This is a key idea: in Poisson Regression, the responses are counts and the parameter $\mu_i$ is the rate that we observe these counts.

We wish to model these count responses $Y_i$ in terms of a vector of predictors $x_i = (1, x_{1i}, ..., x_{pi})^T$. However, we can't directly model the counts, so we need to model a function of the expected value of the $Y_i$ values, or a function of the rate. We can model the rate as a linear combination of predictors called eta.

In general, the linear predictor is

$$
\eta_i = \beta_0 + x_i\beta + Z_i
$$

where $\beta_0$ represents the intercept, and $\beta = (\beta_1, ..., \beta_p)^T$ represents the vector of coefficients of the fixed effects. Furthermore, the responses are assumed to be independent given $\mu_i$ and the random effects are assumed to be independent and identically distributed normal random variables with mean 0 and variance $\sigma_2$ ($(Z_i \stackrel{iid}{\sim} N(0, \sigma^2)$). 

Since we can't model the rate directly, we model a function of the rate by using the following, which is called the log link function (the log function linking the expected value of $Y_i$ with the linear function of our predictors):

$$
\eta_i = log(\mu_i)
$$
or

$$
\mu_i = exp(\eta_i)
$$
Since there is no closed-form formula for $\hat{\beta}$ that can be obtained from the log-liklihood, numerical methods need to be used to solve for $\hat{\beta}$.

This package consists of three functions - run_model, SE_CI, and Hypothesis_Test. The function run_model will fit the Poisson GLMM and return an output of a list of summary statistics that include the estimates for the $\beta$ coefficients, the value of $\sigma^2$, the estimates for the random effects, the test statistics for each of the parameters, whether or not the model converged or not and associated warning messages. The SE_CI function uses bootstrapping to compute estimates for the standard error and confidence intervals for the $\beta$ coefficients and $\sigma^2$. The Hypothesis_Test function uses bootstrapping to test the hypothesis that there is no significant effect of the predictors of interest on the response. 


## Gopher Tortoises

To illustrate the use of the functions in this package, we will analyze sample data that comes from a study of upper respiratory tract infections on the survival of wild gopher tortoises. Using the functions in this package, we will fit a Poisson GLMM to the Gopher Tortoise data set, and describe the relationship between increasing seroprevalance of upper respiratory tract infections in Gopher Tortoises on the number of shells found (i.e., the number of dead tortoises) after accounting for the area of each site. Before doing that it is important to analyze the data ourselves.   

## Exploratory Data Analysis

The data set has 7 variables; Site, Year, Shells, Type, Area, Density, and Prev. The first row of the data is shown here:

```{r, echo = FALSE}
tortoise %>%
  head(n = 1L) %>%
  knitr::kable()
```

The response variable of our model is the Shells variable which measures the number of dead tortoises within each site. We want to study the relationship between this variable and both the Prev and Area variables. Before fitting the model we should explore this relationship further. 

```{r, echo = FALSE, fig.width= 7, fig.height=4}
tortoise$year = cut(tortoise$year, 
                    breaks = c(0, 2004.5, 2005.5, 2007), 
                    labels = c("2004", "2005", "2006"))

# colourblind friendly palette
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#550994", "#882255")

ggplot(data = tortoise, aes(x = Area, y = shells, color = Site, shape = year)) +
  geom_point() +
  theme_bw() + 
  theme(legend.position = "top") + 
  labs(title = "Area of Site vs. Number of Shells", 
       x = "Area of Site", 
       y = "Number of Shells") + 
  guides(shape = guide_legend(title = "Year")) +
  scale_color_manual(values = cbbPalette)

ggplot(data = tortoise, aes(x = prev, y = shells, color = Site, shape = year)) +
  geom_point() +
  theme_bw() + 
  theme(legend.position = "top") + 
  labs(title = "Seroprevalence vs. Number of Shells", 
       x = "Seroprevalence", 
       y = "Number of Shells") + 
  guides(shape = guide_legend(title = "Year")) +
  scale_color_manual(values = cbbPalette)
```

These plots show the relationship between these variables. We can see that both variables have a slight positive correlation with Shells, but Prev has more influence on Shells. One particular site that is particularly telling is the "CF" site. We can see the first two year have low seroprevalence and there are 0 Shells, and then in 2006 when seroprevalence increases to around 45, shells increases to 3. There are a few outliers.

These outliers can be explained by this table below. It shows that over 2003 there was an especially high level of Shells even with an average prevalence over the sites lower than the following years.

```{r, echo = FALSE}
shells_by_year = tortoise %>%
  group_by(year) %>%
  summarize("Total Shells" = sum(shells),
            "Average Prevalence" = mean(prev))

shells_by_year %>%
  knitr::kable()
```

This next table shows total shells across all years by site, as well as the average prevalence across the years. It shows that the highest number of shells belongs to the site with the highest average prevalence, and the lowest number of shells with the lowest average prevalence.

```{r, echo = FALSE}
shells_by_site = tortoise %>%
  group_by(Site) %>%
  summarize("Total Shells" = sum(shells),
            "Average Prevalence" = round(mean(prev), digits = 3)) %>%
  arrange(desc(`Total Shells`))

shells_by_site %>%
  knitr::kable()
```

## Data Processing

Before fitting the model, we clean up the dataset by adding an ID column and removing the type and density columns as we are not interested in these variables.

```{r}
tortoise <- tortoise %>%
    rowid_to_column("ID") %>%  # add ID column
    select(-type, -density)    # delete type and density columns
```


## Fit Model

The run_model function in this package is used to fit the tortoise data to a poisson glmm. The first parameter needed for this function is the data, and the second paramter is the name of the dataset used. The function also takes the fitted values, $\hat{\eta_{ij}}$ obtained from running the model (these values don't include the random effects). After sampling the random effects within the bootstrap function, we are able to set $\eta_{ij}^* = \beta_0 + \beta_1x_{ij1} + log(x_{i2}) + z_i^*$ and refit the model to obtain bootstrapped estimates of $\beta_0$ and $\beta_1$. This is discussed in more detail below. 

```{r}
# Fit model to gopher tortoises data
tortoise_fit <- run_model(data = tortoise, example = "tortoise")

# Obtain the fitted values
x1 <- tortoise$prev
x2 <- tortoise$Area

fitted_values <- tortoise_fit$beta[[1]] + tortoise_fit$beta[[2]] * x1 + log(x2) 
# fitted values without random effects

```

We can investigate the summary output returned from this function:

```{r}

## Components

# 1) Estimated beta parameters
tortoise_fit$beta

# 2) Test statistics for each of the beta parameters
tortoise_fit$test_stat

# 3) Estimated variance parameter
tortoise_fit$sigmasq

# 4) Estimated random effects
tortoise_fit$re

# 5) Convergence check
tortoise_fit$convergence

# 6) Associated Messages
tortoise_fit$messages

```



## Bootstrap

After fitting the model using the run_model function, we will now use the function SE_CI from this package to find the standard errors and confidence intervals. This function implements a parametric bootstrap by simulating the random effects from $N(0, \sigma^2)$, since the random effects are assumed to be independent and identically distributed normal random variables with mean 0 and variance $\sigma^2$ $(Z_i \stackrel{iid}{\sim} N(0, \sigma^2)$.

Below, we implement the SE_CI function using the gopher tortoises data. The function returns a list of 3 tibbles. The tibbles contain the standard error, and upper and lower confidence interval estimates for $\beta_0$, $\beta_1$, and $\sigma^2$.

To run the function, we specify the dataset we are using, the name of the example (in our case, this will be "tortoise"), the fitted_values we computed earlier (keeping in mind that they do not include the random effects), the number of repeated observations (in our case, the m = 3 years are considered as repeated measures), and finally, the name of the response variable of interest as found in our data as a string (we use "shells").

```{r, warning=FALSE, message=FALSE}
# Estimating Standard Errors and Confidence Intervals
se_ci_estimates <- SE_CI(my_data = tortoise, example = "tortoise", 
                         fitted_values = fitted_values, m = 3, response = "shells")
```

Here is a brief description of what the SE_CI function is doing:

Within the function, we first fit our model using the run_model function. For ease, we add the value of $\hat{\beta_1}x_{ij1}$ to the dataset.

Next, we wish to generate 1000 bootstrap samples. The SE_CI function will resample the values of the 10 random effects for each of the 3 years to obtain 1000 bootstrap samples of size 30. Since we have the estimates of the fixed effects from our initial fit of the model, we can compute $\eta_{ij}^* = \beta_0 + \beta_1x_{ij1} + log(x_{i2}) + z_i^*$ for $i=1, ..., 10$ and $j = 1,2,3$. Since $\mu_{ij} = e^{\eta_{ij}}$, we can compute $\mu_{ij}^*$ and simulate new $Y_{ij}^* \sim \text{Poisson}(\mu_{ij}^*)$ for $Y_{ij}$, the number of shells found at each site. We then refit the model to obtain 1000 bootstrapped estimates of $\beta_0^*$, $\beta_1^*$, and $\sigma^2$. The function also checks the convergence each time the model is fitted in the bootstrap and removes the estimates from the bootstrap where the model did not converge. Finally, we compute the standard errors of each estimate, as well as the confidence intervals.

The SE_CI function returns an output of a list of tibbles giving the standard error, lower bound of the confidence interval and upper bound of the confidence interval. Here is the output after running the function on the Gopher Tortoises data:

```{r, echo = false}

se_ci_estimates 

```

We are also interested in performing a significance of regression test using the t-test. Bootstrapping may also be implemented to carry out this test. For our example, we wish to test the null hypothesis that there is no effect of prevalence on the number of shells found at each site:

$$
H_0: \beta_1 = 0 \text{ versus } H_1: \beta_1 \neq 0
$$ 

Since the p-value is the probability that we observe a value of the test statistic as extreme or more extreme than what we observed given that the null hypothesis is true, we need to simulate form the sampling distribution of $\beta_1$ given that the null hypothesis is true. To do this, we repeat the bootstrap process above but use the linear predictor under the null hypothesis, which is:

$$
\eta_{ij} = \beta_0 + log(x_{i2}) + z_i
$$

The function Hypothesis_Test within this package computes and returns the p-value after bootstrapping. The model is fit using the run_model function and a t-value for $\hat{\beta_1}$, the coefficient estimate for the seroprevalence parameter, is obtained. The function then generates bootstrapped samples for the random effects, and computes the value of $\eta_{ij}$ as seen above, allowing for the value of $\mu_{ij}$ to be obtained as well. The values of $Y_{ij}$ are then computed and the model is refit. The function then finds the bootstrapped test statistic values and computes the p-value, which is then returned.

Below, we implement the Hypothesis_Test function using the tortoise data. The arguments this function requires are similar to those of the SE_CI function used earlier.

```{r}

p_value <- Hypothesis_Test(my_data = tortoise, example = "tortoise", 
                           fitted_values = fitted_values, m = 3, response = "shells")
p_value

```

The p-value computed is 0. A p-value won't be exactly 0 since it is the area under the t-distribution curve, so we can assume that the p-value obtained is very small. This makes sense since in the original fit of the model, the t-statistic obtained for the parameter estimate for the prevalence is close to 5, which is very large. Since we have a small p-value, we can reject the null hypothesis and say that the seroprevalence of upper respiratory tract infections in Gopher Tortoises does have an effect on the number of shells found after accounting for the area of each site.

```{r, echo = FALSE, message=FALSE, warning = FALSE}
my_data = tortoise
fit = run_model(my_data, example = "tortoise")

  # extract test statistic for prevalence
t_value <- fit$test_stat[[2]]

  # generate bootstrap samples under null hypothesis
B <- 1000

bootstrap <- tibble(B = 1:B) %>%
    crossing(my_data) %>%
    mutate(z = rep(rnorm(n()/3, mean = 0, sd = sqrt(fit$sigmasq)), each = 3), # resample the random effects from N(0, sigmasq)
           eta = fit$beta[[1]] + log(Area) + z,  # eta_ij* = beta_0 + log(xi2) + zi* (no beta_1 this time)
           shells = rpois(n(), lambda = exp(eta))) %>%  # Yij*~Poisson(lambda) where lambda = mu_ij* = exp(eta_ij*)
    nest_by(B) %>%
    summarize(values = suppressMessages(run_model(data = data)), .groups = "drop")  # refit

  # check convergence
Convergence = bootstrap %>%
    filter(row_number() %% 6 == 5) %>%
    unnest(cols = values) %>%
    filter(values == -1) %>%
    select(B)

  # delete rows with unconverged estimates
bootstrap = bootstrap %>%
    rows_delete(Convergence)

  # Compute p-value

  # first extract bootstrapped test statistics
p_value <- bootstrap %>%
    select(values) %>%
    filter(row_number() %% 6 == 4) %>%  # run_model has a list of 6 outputs, 0 to take the test statistics
    unnest(cols = values) %>%
    filter(row_number() %% 4 == 2)  # 2 to take the test-stat for prev
  
my_plot = ggplot(data = p_value, aes( x = values)) + 
         geom_density() +
         geom_vline(xintercept = t_value, col = "red") + 
         ylab("Bootstrap Sampling Distribution (Null)") + 
         xlab("Slope")

my_plot
```
